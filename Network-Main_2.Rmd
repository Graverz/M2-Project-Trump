---
title: "SDS 2019 - M2: Group Assignment"
author: "Andreas, Simon, Jess, Lars"
date: "14/9/2019"
output:
  html_document:
    code_folding: hide
    theme: flatly
    toc: yes
    toc_float:
      collapsed: no
---
# Assignment
**Introduction**
In the M2 sessions, online courses and assignments you learned how to work with relational data and text in different contexts and on various levels of aggregation. These skills allow you to access and process a vast range of data. The analysis of textual as well as relational data indeed has a lot of synergies. Often measures of relationships can be created based on text data. Likewise, often the analysis of text data can be enhanced by analyzing the relationship between keywords, documents, and the like. 

Now it’s time to get creative. We would like you to carry out an own analysis on a self-chosen topic (and on self-chosen data). This analysis should be interesting and informative and contain elements and methods from network analysis as well as natural language processing. The balance between the two fields is up to you, as long as the application of both is non-trivial.

**Task description**

**Data & Problem identification**
In this exercise, you are asked to choose and obtain a dataset you consider interesting and appropriate for the tasks required. 
You are welcome to use existing datasets for [language](https://github.com/niderhoff/nlp-datasets) and [networks](https://snap.stanford.edu/data/) but at this stage you could also consider getting your own data (e.g. Twitter API, Instagram, news repositories etc.)

The data should be large enough and of proper granularity to be interesting for NLP and network analysis techniques. If you are in doubt, please reach out.

What we expect you to do:

* Identify an interesting problem that can be tackled using data science techniques applied to natural language and networks.
* Select and obtain relevant data to do so.
* Clean and manipulate the data to make it useful.
* Carry out an exploratory data analysis to provide intuition into the content of the data, and interesting relationships to be found in it.
* Use unsupervised ML techniques to discover relationships within the data such as interesting topics or latent network structures.
* Use supervised ML techniques to create models that predict an outcome of interest.
* Document your workflow in a reconstructable manner.
* Report your findings in an accessible manner.

**Analysis pipeline** 

The analysis to be carried out by you has to contain elements of **data manipulation**, **exploration**, **unsupervised** and **supervised ML** as applied to **relational** and **language data**.

In the best case, you combine network data with language elements. Twitter is a good (and easy) example, as you can, for instance, combine mention-networks with sentiments expressed in the tweets. The article below is a creative example of that (with a rather small NLP part).

[Liu, Z., & Weber, I. (2014, November)](https://link.springer.com/chapter/10.1007/978-3-319-13734-6_25). Is Twitter a public sphere for online conflicts? A cross-ideological and cross-hierarchical look. In International Conference on Social Informatics (pp. 336-347). Springer, Cham.

* Definition of a problem statement and a short outline of the implementation 
* Description of data acquisition / how it was collected (by you or the publisher of the data) 
* Data preparation (general)
    * Data cleaning (if needed)
    * Recoding (label encoding, dummy creation etc.)
    * Merging and wrangling (if needed)
* Missing data imputation (if applicable and deemed relevant) 
* Network Data - preparation
    * Extraction and formatting
    * Creation of functional graphs with relevant attributes
* NLP - preparation
    * Extraction & Cleaning 
    * Tokenization
    * Filtering & Lemmatization / Stemming (if needed)
* Network analysis 
    * Calculation of relevant indicators on different levels / EDA
    * Projection (in the case of bipartite graphs)
    * Identification of community structures
* NLP
    * EDA / simple frequency-based analysis
    * Simple vectorization (BoW, Tf-idf)
    * Topic modelling / Clustering (LDA / LSA)
    * Embedding-model based vectorization (Word2Vec, Fasttext, GloVe)
* Supervised / Unsupervised ML
    * Try to link your results from network analysis or NLP with a more traditional ML problem.

**Many of the steps are optional.** So choose which methods you deem helpful and relevant to explore your chosen problem.

**Note:** Quality > Quantity. Consider which analysis, summarization, and visualization adds value. Excessive and unselective outputs (e.g. running 20 different models without providing a reason for, providing all possibilities of different plots without discussing and evaluating the insights gained from it) will not be considered helpful but rather distracting.

**Some inspirational examples (non-binding, and non-exhaustive):**

1. You obtain a dataset with tweets on a current debate (e.g. #MeeToo) and try to map the discourse. 
    * You perform “naive” NLP, counting handles, hashtags, basic plotting etc. to get some overview. 
    * You perform “out-of-the-box” sentiment analysis and plot tweets on a map, colouring by sentiment.
    * You perform topic modelling and identify the sub-discussions. 
    * Isolating handles/retweets, you identify some interaction patterns, use network indicator to identify thought leaders or conflicting communities as well as people that try to negotiate between positions.
2. You obtain a bibliographic dataset on a field of study (or from an entity such as a university) of interest, e.g., from scopus. 
    * You perform a network analysis on different levels of aggregations, identifying key publications, scientists etc.
    * You run a topic model to identify relevant discourses.
    * You might then answer questions such as: Did the discourses change over time? In case so, who or what drives these changes?

**Documentation and Deliverables**

You are asked to hand in two different report formats, namely:

1. Functional computational notebook
2. Stakeholder report 

**Computational Notebook**
The notebook targets a **machine-learning literate audience**. Here you can go deeper into the technical details and method considerations. Provide thorough documentation of the whole process, the used methods. Describe the **intuition** behind the selected and used methods, **justify** choices made, and **interpret** results (e.g. Why scaling? Why splitting the data? Why certain tabulations and visualizations? What can be seen from ... ?, How did you select a particular algorithm? Why did you scale features in one way or another?). 
Please provide the notebook as a PDF ot HTML (Knittered from rmd or converted ipynb, when HTML zipped) with a public link to a functional Colab version (test it beforehand in incognito/private mode of your browser)

**Stakeholder Report**
The stakeholder report (simple PDF, no code) summarises the analysis for a **non-technical audience**. Here you don't need to discuss alternative approaches to standardization and alike. Instead, you should try to explain the analysis and results, emphasizing its meaning and interpretation. Imagine it as a report of the **project outcome**, as you would explain it to a general audience.to 
Aim at a length of **not more than 5 pages**, including tables & visualizations.

**Finally**

* Submission deadline: **Thursday, 24 October 2019, 11:55pm**. 
* Evaluation seminar: **Tuesday, 28. October 2019**. Technical details regarding the submission and evaluation of the assignment will be sent out in the beginning of next week.
* We will send out a doodle to accommodate your schedules as well find proper alignment for R vs Python groups.
* In case of trouble/issues/questions, please write on Slack. When possible, try to get help from your classmates in the *#module2-assignment-clinique channel*. We will also take a look at what's happening there.

# Introduction - Climate





```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(tidygraph)
library(ggraph)
library(geosphere)
library(igraph)
library(maps)
library(tm)
library(gridExtra)
library(knitr)
library(kableExtra)

air <- read_csv("https://raw.githubusercontent.com/datasets/airport-codes/master/data/airport-codes.csv", na = character())

base <- "#91cf60"
```


# Electric Air Travle (Network)
First we start by preparing the data. And we look at it by continent and type

```{r message=FALSE, warning=FALSE}
cc <- strsplit(air$coordinates, "[,]")

air <- air %>%  
  mutate(lon = unlist(cc)[2*(1:length(air$coordinates))-1],
         lat = unlist(cc)[2*(1:length(air$coordinates))  ])

air$cont <- ifelse(air$continent == "NA", "North America", 
            ifelse(air$continent == "SA", "South America",
            ifelse(air$continent == "EU", "Europe",
            ifelse(air$continent == "AS", "Asia",
            ifelse(air$continent == "AF", "Africa",
            ifelse(air$continent == "OC", "Oceania", "Antartica"))))))

air %>% 
  group_by(type) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  kable() %>% 
  kable_styling(c("bordered", "condensed","hover"), full_width = F)
```

From this we see that there is alot of small airports, almost 34000. As we're interested in calculating the distance between every airport, and therefore square the number of airports, this is too much as it will give 1.155.864.004 distances to calculate. We therefore only look at large airports as this will give us $613^2$ which is 375.769 distances to caluclate. If we look at where these airports are located we find: 

```{r}
air %>% 
  filter(type == "large_airport") %>% 
  group_by(cont) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  kable() %>% 
  kable_styling(c("bordered", "condensed","hover"), full_width = F)
```

We now calculate all the distances in kilometers by the coordinates and save them for use in the networks.

```{r message=FALSE, warning=FALSE}
air_large <- air %>% 
  filter(type == "large_airport") %>% 
  group_by(name) %>% 
  summarise_all(.funs = first)

m <- air_large %>% select(lat, lon) %>% 
  mutate(lat = as.numeric(lat),
         lon = as.numeric(lon)) %>% 
  as.matrix()


dista <- distm(m, fun = distHaversine) %>% as.data.frame()

names(dista) <- air_large$name
dista$name <- air_large$name

mm <- dista %>% gather(to, value,-name) %>% mutate(value = value /1000) %>% rename(from = name)

```

## Visualisation
But before we plot some networks we find plot the airports on a world map just to see what we're dealing with

```{r fig.height=4, fig.width=10}

WorldData <- ggplot2::map_data('world') %>% filter(region != "Antarctica") %>% fortify()


ggplot() +
  geom_map(data = WorldData, map = WorldData,
           aes(x = long, y = lat, group = group, map_id=region),
           fill = "white", colour = "#7f7f7f", size=0.5) + 
  coord_map("rectangular", lat0=0, xlim=c(-180,180), ylim=c(-60, 90)) +
  geom_point(data=air_large, aes(x=as.numeric(lat), y=as.numeric(lon), color=cont), alpha=0.7) +
    scale_color_brewer(palette="RdYlGn") +
    scale_y_continuous(breaks=c()) +
    scale_x_continuous(breaks=c()) +
    labs(fill="legend", title="Map of large airports", x="", y="") +
    theme_minimal()
```

Our challenge is to find how the world is connected if we loose access to long distance commercial flights and have to replace all air travle with short rance battery powered flights. So we look at this with some different distances. If the distance is too short there is no connections, and if it's too long everything is connected. We start out with showing how the airports of the world starts clustering as we slowly increase the distance in the network from 50 to 750 km.


```{r fig.height=10, fig.width=10}
df <- data.frame(id        = air_large$name, 
                 region    = air_large$cont,
                 short     = air_large$ident,
                 elevation = air_large$elevation_ft,
                 country   = air_large$iso_country)

air_n100 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 100) %N>%
  filter(!node_is_isolated())

air_n200 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 200) %N>%
  filter(!node_is_isolated())

air_n300 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 300) %N>%
  filter(!node_is_isolated())

air_n400 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 400) %N>%
  filter(!node_is_isolated())

air_n500 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 500) %N>%
  filter(!node_is_isolated())

air_n600 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 600) %N>%
  filter(!node_is_isolated())

air_n700 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 700) %N>%
  filter(!node_is_isolated())

air_n800 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 800) %N>%
  filter(!node_is_isolated())

air_n900 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 900) %N>%
  filter(!node_is_isolated())

air_n1000 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 1000) %N>%
  filter(!node_is_isolated())

air_n1500 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 1500) %N>%
  filter(!node_is_isolated())

air_n2000 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 2000) %N>%
  filter(!node_is_isolated())

air_n2500 <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 2500) %N>%
  filter(!node_is_isolated())

```



## Networks {.tabset .tabset-fade .tabset-pills}

### kk

```{r fig.height=9, fig.width=12}
fri_pp <- list()

for (i in 1:15) {
  a <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < (i*100/2)) %N>%
  filter(!node_is_isolated())
  
  fri_pp[[i]] <- ggraph(a, layout = "kk") +
     geom_edge_link(alpha=0.9) + 
     geom_node_point(aes(color=region), size=2, alpha=0.6, show.legend = F) +
     scale_color_brewer(palette="RdYlGn") +
     theme_void() + labs(title=paste0(i*100/2, " Km seperation"))
}

fri_pp$nrow <- 3
do.call(grid.arrange, fri_pp)
```


### stress

```{r fig.height=9, fig.width=12}
fri_pp <- list()

for (i in 1:15) {
  a <- tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < (i*100/2)) %N>%
  filter(!node_is_isolated())
  
  fri_pp[[i]] <- ggraph(a, layout = "stress") +
     geom_edge_link(alpha=0.9) + 
     geom_node_point(aes(color=region), size=2, alpha=0.6, show.legend = F) +
     scale_color_brewer(palette="RdYlGn") +
     theme_void() + labs(title=paste0(i*100/2, " Km seperation"))
}

fri_pp$nrow <- 3
do.call(grid.arrange, fri_pp)
```


## Network characteristics

In plot we see that around 300 km. some serious clusters start forming in US and Europe. This is assuring that with a small range of 300 km we would still be able to travle most of Europe, but it wouldn't be possible to make the journey to US. Even at 750 there is still no connection between US and Europe, even though europe is starting to have great connections in asia and the middleeast.

We can plot some charateristics of the network:

```{r fig.height=4, fig.width=13}
network <- as.character(c(seq(100,1000, by=100),1500,2000,2500)) %>% as.factor() %>% reorder(sort(as.numeric(.)))

lll <- list(air_n100, air_n200, air_n300, air_n400,air_n500, air_n600, air_n700, air_n800,air_n900,air_n1000, air_n1500,air_n2000,air_n2500)


dat <- data.frame(network,
                  densities      = unlist(lapply(lll, edge_density)),
                  transitivities = unlist(lapply(lll, transitivity)),
                  components     = unlist(lapply(lll, count_components)),
                  mortifs        = unlist(lapply(lll, count_motifs))/1000,
                  diameter       = unlist(lapply(lll, diameter)),
                  cliques        = unlist(lapply(lll, clique_num)))

dat %>% 
  gather(variable, value, -network) %>% 
  ggplot(aes(network, value)) + 
  geom_col(position="dodge", width=0.9, fill=base) + 
  labs(title="Network level characteristics", x="", y="", fill="distance") + 
  facet_wrap(~variable, scales="free", nrow=2)
  
```

We see here that

- cliques: The longer distance creates more and more cliques
- components: we get fewer and fewer parts of the whole plot, and would end up in just one component if just the distance is large enough
- densities: We see a smooth increase in density as we increase range
- diameter: 
- mortifs:
- transitivities: 

as the network is non directed it means that reciprocity is 1 for all the networks.


## Network centrality
We are also interested in seeing what airports are the most central, we utilize three centrality measures and do the analisis on 3 networks, 300, 500 and 1000 km.

```{r fig.height=10, fig.width=10}


a <- air_n300 %N>% 
  mutate(centrality_dgr = centrality_degree(mode = "in"),
         centrality_eigen = centrality_eigen(),
         centrality_betweenness = centrality_betweenness(),
         id2 = substr(id, 1, 30)) %N>% 
  as.tibble() %>%
  gather(variable,value, -id,-id2,-region,-short,-country, -elevation) %>% 
  group_by(variable) %>% 
  top_n(40, wt = value) %>% 
  ggplot(aes(reorder_within(id2, value, variable), value, fill=region)) + 
  geom_col(width=0.8) + 
  scale_x_reordered() +
  scale_fill_brewer(palette="RdYlGn") +
  coord_flip() + 
  theme(axis.text.y = element_text(size=4)) + 
  facet_wrap(~variable, scales = "free") +
  labs(x="",y="",title="Centrality of airports within 300 km")

b <- air_n500 %N>% 
  mutate(centrality_dgr = centrality_degree(mode = "in"),
         centrality_eigen = centrality_eigen(),
         centrality_betweenness = centrality_betweenness(),
         id2 = substr(id, 1, 30)) %N>% 
  as.tibble() %>%
  gather(variable,value, -id,-id2,-region,-short,-country, -elevation) %>% 
  group_by(variable) %>% 
  top_n(40, wt = value) %>% 
  ggplot(aes(reorder_within(id2, value, variable), value, fill=region)) + 
  geom_col(width=0.8) + 
  scale_x_reordered() +
  scale_fill_brewer(palette="RdYlGn") +
  coord_flip() + 
  theme(axis.text.y = element_text(size=4)) + 
  facet_wrap(~variable, scales = "free") +
  labs(x="",y="",title="Centrality of airports within 500 km")

c <- air_n1000 %N>% 
  mutate(centrality_dgr = centrality_degree(mode = "in"),
         centrality_eigen = centrality_eigen(),
         centrality_betweenness = centrality_betweenness(),
         id2 = substr(id, 1, 30)) %N>% 
  as.tibble() %>%
  gather(variable,value, -id,-id2,-region,-short,-country, -elevation) %>% 
  group_by(variable) %>% 
  top_n(40, wt = value) %>% 
  ggplot(aes(reorder_within(id2, value, variable), value, fill=region)) + 
  geom_col(width=0.8) + 
  scale_x_reordered() +
  scale_fill_brewer(palette="RdYlGn") +
  coord_flip() + 
  theme(axis.text.y = element_text(size=4)) + 
  facet_wrap(~variable, scales = "free") +
  labs(x="",y="",title="Centrality of airports within 1000 km")

grid.arrange(a,b,c)
```

We see that in the first two networks in eigen centrality europe dominates, that is because the airports are close here, but with 1000 km we see how that the greater number of airports in the us counts high. we also here see that in the betweenness measure some airports in Asia scores high. This is because they end up ad the connectors between Europe and asia.

What does this mea? well, if planes got restricted to 1000km, some of the most well connected airports would be in US (Louisville International Standiford Field and Nashville International Airport), but to travle the longest distance, then airports in Asia (Koltsovo Airport in Yekaterinburg and Astana International Airport) would be the most central.

With a medium distance of 500km EBBR - Brussels Airport does pretty well


## Overall network 300km
We see here the network of 300km, there is a still manny places you can't reach

```{r fig.height=5, fig.width=10}
air_n300 %>% 
  ggraph(layout = "kk") + 
  geom_edge_link(alpha=0.5) + 
  geom_node_point(aes(color=region),alpha=0.85, size=7) +
  scale_color_brewer(palette="RdYlGn") +
  geom_node_text(aes(label = short), size=1.5, color="white") +
  labs(title="Color by centrality - 300 km") + theme_void()
```


### Centrality
Here we plot the networks with centrality as color and size


```{r fig.height=5, fig.width=10}
a <- air_n100 %N>% 
  mutate(centrality_dgr = centrality_degree(mode = "in")) %>% 
  ggraph(layout = "mds") + 
  geom_node_point(aes(color=centrality_dgr, size=centrality_dgr), alpha=0.6) +
  geom_edge_link(alpha=0.2, color="white") +
  scale_size(range=c(3,10), breaks = c(1,3,6)) +
  scale_color_viridis_c(guide = "legend", breaks = c(1,3,6), option = "inferno") + 
  #geom_node_text(aes(label = short), size=0.7, color="white") +
  labs(title="Color by centrality - 100 km") + theme_void()+ theme(legend.position="bottom")

b <- air_n300 %N>% 
  mutate(centrality_dgr = centrality_degree(mode = "in")) %>% 
  ggraph(layout = "mds") + 
  geom_node_point(aes(color=centrality_dgr, size=centrality_dgr), alpha=0.6) +
  geom_edge_link(alpha=0.1, color="white") +
  scale_size(range=c(3,10), breaks = c(1,9,18)) +
  scale_color_viridis_c(guide = "legend", breaks = c(1,9,18), option = "inferno") + 
  #geom_node_text(aes(label = short), size=0.7, color="white") +
  labs(title="Color by centrality - 300 km") + theme_void()+ theme(legend.position="bottom")

c <- air_n500 %N>% 
  mutate(centrality_dgr = centrality_degree(mode = "in")) %>% 
  ggraph(layout = "mds") + 
  geom_node_point(aes(color=centrality_dgr, size=centrality_dgr), alpha=0.6) +
  geom_edge_link(alpha=0.05, color="white") +
  scale_size(range=c(3,10), breaks = c(1,15,30)) +
  scale_color_viridis_c(guide = "legend", breaks = c(1,15,30), option = "inferno") + 
  #geom_node_text(aes(label = short), size=0.7, color="white") +
  labs(title="Color by centrality - 500 km") + theme_void()+ theme(legend.position="bottom")

grid.arrange(a,b,c,nrow = 1)
```


### assortativity 
We calculate the assortativity of country and region

```{r fig.height=3, fig.width=10}
r <- c("value", "type", "network")
r1 <- c(assortativity(air_n100, V(air_n100)$region),               "REGION",    "100")
r2 <- c(assortativity(air_n100, V(air_n100)$country),              "COUNTRY",   "100")
r3 <- c(assortativity(air_n200, V(air_n200)$region),               "REGION",    "200")
r4 <- c(assortativity(air_n200, V(air_n200)$country),              "COUNTRY",   "200")
r5 <- c(assortativity(air_n300, V(air_n300)$region),               "REGION",    "300")
r6 <- c(assortativity(air_n300, V(air_n300)$country),              "COUNTRY",   "300")
r7 <- c(assortativity(air_n400, V(air_n400)$region),               "REGION",    "400")
r8 <- c(assortativity(air_n400, V(air_n400)$country),              "COUNTRY",   "400")
r9 <- c(assortativity(air_n500, V(air_n500)$region),               "REGION",    "500")
r10 <- c(assortativity(air_n500, V(air_n500)$country),              "COUNTRY",  "500")
r11 <- c(assortativity(air_n600, V(air_n600)$region),               "REGION",   "600")
r12 <- c(assortativity(air_n600, V(air_n600)$country),              "COUNTRY",  "600")
r13 <- c(assortativity(air_n700, V(air_n700)$region),               "REGION",   "700")
r14 <- c(assortativity(air_n700, V(air_n700)$country),              "COUNTRY",  "700")
r15 <- c(assortativity(air_n800, V(air_n800)$region),               "REGION",   "800")
r16 <- c(assortativity(air_n800, V(air_n800)$country),              "COUNTRY",  "800")
r17 <- c(assortativity(air_n900, V(air_n900)$region),               "REGION",   "900")
r18 <- c(assortativity(air_n900, V(air_n900)$country),              "COUNTRY",  "900")

df <- as.data.frame(rbind(r1,r2,r3,r4,r5,r6,r7,r8,r9,r10,r11,r12,r13,r14,r15,r16,r17,r18))
names(df) <- r
df$value <- as.numeric(as.character(df$value))

ggplot(df, aes(network, value)) + 
  geom_col(fill=base) + 
  facet_wrap(~type, nrow=1) + 
  labs(title="Assortativity by country and region")
```

We see that assortivity falls fast as distance increase, this makes sense as countries are small areas, and therefore it's expected to be most clusteded by country when the networks are small




```{r}
df <- data.frame(id        = air_large$name, 
                 region    = air_large$cont,
                 short     = air_large$ident,
                 elevation = air_large$elevation_ft,
                 country   = air_large$iso_country)

tbl_graph(edges = mm, nodes = df) %E>% 
  filter(value < 3000) %>%
  ggraph(layout = "stress") + 
  #geom_edge_link(alpha=0.5) + 
  geom_node_point(aes(color=region), size=2) +
  scale_color_brewer(palette="RdYlGn") +
  #geom_node_text(aes(label = id), size=2) +
  labs(title="map") + theme_void()
```







# Trump's view on climate (NLP)









# Conclusion





---
title: "M2_group"
author: "Jess"
date: "21 oktober 2019"
output:  
    html_document:
    code_folding: hide
    theme: flatly
    toc: yes
    toc_float:
    collapsed: no
---

```{r message=FALSE, warning=FALSE}
if (!require("pacman")) install.packages("pacman") # package for loading and checking packages :)
pacman::p_load(tidyverse, # Standard datasciewnce toolkid (dplyr, ggplot2 et al.)
               magrittr, # For advanced piping (%>% et al.)
               tidytext, # For text analysis
               tm,
               topicmodels,
               RJSONIO,
               lubridate,
               wordcloud,
               SnowballC,
               textdata,
               rsample,
               quanteda,
               text2vec,
               uwot,
               dbscan,
               caret
)
```

# Bag-o-Words

```{r message=FALSE, warning=FALSE}
trump_tweet <- read_csv("https://raw.githubusercontent.com/Graverz/M2-Project-Trump/master/Trump_tweets.csv")  %>% rename(ID = X1) %>% select(-2)


trump_tidy <- trump_tweet %>%  
  unnest_tokens(word, text) 

trump_tidy %<>%
  add_count(word, name = "nword") %>%
  filter(nword > 1) %>%
    select(-nword)

trump_tidy %<>% 
  group_by(word) %>% 
  filter(n()>10) %>% 
  ungroup()

own_stopwords <- tibble(word= c("t.co", "https", "amp", "rstats","rt"),
                        lexicon = "OWN")

trump_tidy %<>%
  anti_join(stop_words %>% bind_rows(own_stopwords), by = "word") 

trump_tidy %<>%
  mutate(word = word %>% str_remove_all("[^[:alnum:]]") ) %>%
  filter(str_length(word) > 1) 

trump_tidy %>% 
  count(word, sort = TRUE) %>% 
  head(20)

trump_tidy %>% 
  count(word, sort = TRUE) %>% 
  head(20) %>% 
  ggplot(aes(x = word %>% fct_reorder(n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Word Counts", 
       x = "Frequency", 
       y = "Top Words")

```

## Counts

```{r}

grepl("Climate",x = trump_tweet$text) %>% 
sum()

grepl("renewable",x = trump_tweet$text) %>% 
sum()

grepl("tax",x = trump_tweet$text) %>% 
sum()

grepl("fake",x = trump_tweet$text) %>% 
sum()

trump_tidy %>% 
  group_by(ID) %>% 
  filter(word =="climate") %>% 
  count() %>% 
 arrange(desc(n))


trump_tidy %>% 
  group_by(ID) %>% 
  filter(word =="paris" | word =="agreement") %>% 
  count() %>% 
 arrange(desc(n))

trump_tidy %>%  
  filter(word == "hurricane")

```

## visualization

```{r}
trump_top <- trump_tidy %>% 
  count(word, sort = TRUE) %>% 
  head(20)

wordcloud(trump_top$word, trump_top$n, random.order = FALSE, max.words = 50, colors = brewer.pal(8,"Dark2"))
```
## TF-IDF

```{r}
trump_IDF  <- trump_tidy %>% 
  count(ID, word, sort = TRUE) %>% 
  bind_tf_idf(word, ID, n)

trump_IDF %>% head()

trump_IDF %>%
  arrange(desc(tf))
```

# LDA

```{r}

set.seed(1)
trump_dtm <- trump_tidy %>%
  count(ID,word) %>%
  cast_dtm(document = ID, term = word, value = n, weighting = tm::weightTf)

trump_lda <- trump_dtm %>% 
  LDA(k = 7, method = "Gibbs",
      control = list(seed = 1337))

trump_beta <- trump_lda %>% 
  tidy(matrix = "beta") %>%
  group_by(topic) %>%
  arrange(topic, desc(beta)) %>%
  slice(1:10) %>%
  ungroup() 
```

```{r fig.height=12, fig.width=20}
trump_beta %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 2, scales = "free")
```
# Sentiment 

```{r}
#AFINN numeric score 
#bing positive negative label 
#nrc labels words as fear, joy, etc

trump_sentiment <- trump_tidy %>%  
  inner_join(get_sentiments("afinn")) %>% 
  group_by(ID) %>% 
  summarise(sentiment = sum(value)) %>% 
  arrange(sentiment)

trump_sentiment %>% head()

trump_sentiment %>% 
  count(sentiment) %>%  
  arrange(desc(n)) 
```

```{r}
trump_sentiment_b <- trump_tidy %>%  
  inner_join(get_sentiments("bing")) %>% 
  group_by(ID) %>% 
  arrange(sentiment)

trump_sentiment_b %>% head()

trump_sentiment_b %>% 
  count(sentiment) %>%
  filter(sentiment == "negative")%>% 
arrange(desc(n))


```

```{r}
trump_sentiment_nrc <- trump_tidy %>% 
  inner_join(get_sentiments("nrc") %>% filter(sentiment == "fear")) %>% 
  count(word, sort= TRUE)

trump_sentiment_nrc %>%  head()
```

```{r}
trump_sentiment_p <- trump_tidy %>%  
  inner_join(get_sentiments("afinn")) %>% 
  group_by(date) %>% 
  summarise(sentiment = sum(value)) %>% 
   mutate(mood = ifelse(sentiment >= 0, "positive","negative"))

trump_sentiment_p %>% head()

trump_sentiment_p %>% 
  ggplot() +
  geom_line(aes(date, sentiment, color = mood, group=0)) +
  theme_light() +
  scale_color_manual(values = c("positive" = 'blue', "negative" = 'red'))



trump_sentiment_p %>% 
  ggplot(aes(date, sentiment, color = mood)) +
  geom_point() +
  theme_light() +
  scale_color_manual(values = c("positive" = 'blue', "negative" = 'red')) + 
  geom_smooth(aes(group=0))


trump_sentiment_p %>% 
  group_by(week(date), year(date)) %>% 
  summarize(sentiment=sum(sentiment),
            mood = ifelse(sentiment >= 0, "positive","negative"),
            date = first(date)) %>% 
  ggplot(aes(date, sentiment, fill=mood, color=mood)) +
  geom_col(width=1) +
  geom_point() +
  theme_light() +
  scale_color_manual(values = c("positive" = 'blue', "negative" = 'red')) + 
  scale_fill_manual(values = c("positive" = 'blue', "negative" = 'red')) + 
  geom_smooth(aes(group=0), se=F)


```
# GloVe
```{r}
trump_corp <- trump_tweet %>%  corpus(docid_field = "ID", text_field = "text")

trump_toks <- tokens(trump_corp, what = "word") %>%
  tokens_tolower() %>%
  tokens(remove_punct = TRUE, 
         remove_symbols = TRUE)

trump_dfm <- dfm(trump_toks, verbose = TRUE) %>%
  dfm_trim(min_termfreq = 5) %>%
  featnames()

trump_fcm <- fcm(trump_toks, 
                 context = "window", 
                 count = "weighted", 
                 weights = 1 / (1:5), 
                 tri = TRUE)

glove <- GlobalVectors$new(word_vectors_size = 50, vocabulary = featnames(trump_fcm), x_max = 10)

trump_word_vectors <- fit_transform(trump_fcm, glove, n_iter = 20)
```

```{r}
trump_word_vectors %<>% as.data.frame() %>%
  rownames_to_column(var = "word") %>% 
  as_tibble()
trump_word_vectors %>% head()
```

```{r}
trump_tidy2 <- trump_toks %>% 
  dfm() %>% 
  tidy()

trump_vectors <- trump_tidy2 %>%
  inner_join(trump_word_vectors, by = c("term" = "word"))

trump_vectors %>% head()
```

```{r}
trump_vectors %<>%
  select(-term, -count) %>%
  group_by(document) %>%
  summarise_all(mean)

trump_vectors_umap <- umap(trump_vectors %>% column_to_rownames("document"), 
                       n_neighbors = 15, 
                       metric = "cosine", 
                       min_dist = 0.01, 
                       scale = TRUE,
                       verbose = TRUE) %>% 
  as.data.frame()

trump_vectors_umap %>% 
  ggplot(aes(x = V1, y = V2)) + 
  geom_point(shape = 21, alpha = 0.25) 
```


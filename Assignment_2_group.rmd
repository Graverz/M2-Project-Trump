---
title: "M2_group"
author: "Jess"
date: "21 oktober 2019"
output:  
    html_document:
    code_folding: hide
    theme: flatly
    toc: yes
    toc_float:
    collapsed: no
---
Colab:
https://colab.research.google.com/github/SDS-AAU/M1-2019/blob/master/notebooks/exercises/M1_2_dataviz_ex1.ipynb#sandboxMode=true&scrollTo=FOZeA_izCc3C
Trump Tweets:
http://www.trumptwitterarchive.com/


```{r message=FALSE, warning=FALSE}
if (!require("pacman")) install.packages("pacman") # package for loading and checking packages :)
pacman::p_load(tidyverse, # Standard datasciewnce toolkid (dplyr, ggplot2 et al.)
               magrittr, # For advanced piping (%>% et al.)
               tidytext, # For text analysis
               tm,
               topicmodels,
               RJSONIO,
               lubridate,
               wordcloud,
               SnowballC,
               textdata,
               rsample,
               quanteda,
               text2vec,
               uwot,
               dbscan,
               caret,
               kableExtra,
               mice,
               glmnet,
               rsample,
               yardstick
)

set.seed(1)
```

# Bag-o-Words

President Donald Trump’s Tweets from 2017-2018 is downloaded and the tokenized. Afterwards the data is cleaned and manipulated.
- Tweets with only one word is filtered away
- Words that only appear 8 times and less are left out. 
- Defined stop words are removed 
- Words with only one letter is filtered away
- Characters containing only numeric values are removed

```{r message=FALSE, warning=FALSE}
trump_tweet <- read_csv("https://raw.githubusercontent.com/Graverz/M2-Project-Trump/master/Trump_tweets.csv")  %>% rename(ID = X1) %>% select(-2)

trump_tidy <- trump_tweet %>%  
  unnest_tokens(word, text) 

trump_tidy <- trump_tweet %>%  
  unnest_tokens(word, text) 
trump_tidy %<>%
  add_count(word, name = "nword") %>%
  filter(nword > 1) %>%
    select(-nword)
trump_tidy %<>% 
  group_by(word) %>% 
  filter(n()>8) %>% 
  ungroup()
own_stopwords <- tibble(word= c("t.co", "https", "amp", "rstats","rt", "", "a.m", "p.m", "a.g"),
                        lexicon = "OWN")
trump_tidy %<>%
  anti_join(stop_words %>% bind_rows(own_stopwords), by = "word") 

trump_tidy %<>%
  mutate(word = word %>% str_remove_all("[^[:alnum:]]") ) %>%
  filter(str_length(word) > 1) 

trump_tidy %<>%
  anti_join(stop_words %>% bind_rows(own_stopwords), by = "word") 
trump_tidy %<>%
  mutate(word = word %>% removeNumbers(ucp=F)) %>%
  filter(str_length(word) > 1) 


```

The most used words are depicted in the table and graph below.

```{r}
kable(trump_tidy %>% 
  count(word, sort = TRUE) %>% 
  head(20)) %>% 
  kable_styling(c("condensed","bordered","striped","hover"), full_width = F, position = "left") %>% 
 add_header_above(c("Most used words" =2))

trump_tidy %>% 
  count(word, sort = TRUE) %>% 
  head(20) %>% 
  ggplot(aes(x = word %>% fct_reorder(n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Word Counts", 
       x = "Frequency", 
       y = "Top Words")
```

## Visualization

A wordcloud of the twenty most used words in Donald Trump's Tweets (scaled by number of uses)

```{r}
trump_top <- trump_tidy %>% 
  count(word, sort = TRUE) %>% 
  head(20)

wordcloud(trump_top$word, trump_top$n, random.order = FALSE, colors = brewer.pal(8,"Dark2"))
```
## TF-IDF

The easiest approach to add statistical information on frequency is to apply tf-idf weights. TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.

TF - ‘Term frequency’: The proportion of words in a text that are that term. TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).
IDF - inverse document frequency: The weight of how common a term is across all documents. Measures how important a term is. The more often a word appears in a text the less weight it receives. IDF = Log(Total number of tweets / total number of tweets where the word appears)

As it can be seen Trump is very excited for jobs.

```{r}
trump_IDF  <- trump_tidy %>% 
  count(ID, word, sort = TRUE) %>% 
  bind_tf_idf(word, ID, n)

trump_IDF %>% arrange(desc(tf)) %>%  head(5)

```

# LDA

The goal of topic modeling is to find some significant thematically related terms/topics in some unstructured textual data, which is measured as patterns of word co-occurrence. The basic components of topic models are documents, terms, and topics. Latent Dirichlet Allocation (LDA) is an unsupervised learning method. It finds different topics underlying a collection of documents (where each document is a collection of words).
LDA seeks to find groups of related words. It is an iterative, generative algorithm. Here are the two main steps: 
1) During initialization, which is that each word will be assigned to a random topic which the model puts together. 2) The algorithm goes through each word iteratively and then reassigns the word to a specific topic with the following considerations:
The probability that the word belongs to a specific topic(Beta)
The probability that the document will be generated by a topic (Gamma)
The idea behind the LDA topic model is that words belonging to a topic appear together in a specific document. The algorithm tries to model each document as a mixture of topics and each topic as a mixture of words. After this it’s possible to use the probability that a word belongs to a particular topic to classify it accordingly.

LDA is used to find topics in Donald Trump's Tweets. Different amounts of topics where tried and 9 topics yielded the most distinct and clear topics. The topics and the appropriate categories are shown below.

```{r fig.height=12, fig.width=20}
trump_dtm <- trump_tidy %>%
  count(ID,word) %>%
  cast_dtm(document = ID, term = word, value = n, weighting = tm::weightTf)

trump_lda <- trump_dtm %>% 
  LDA(k = 9, method = "Gibbs",
      control = list(seed = 1337))

trump_beta <- trump_lda %>% 
  tidy(matrix = "beta") %>%
  group_by(topic) %>%
  arrange(topic, desc(beta)) %>%
  slice(1:10) %>%
  ungroup() 

trump_beta %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>% 
mutate(topic = recode(topic,
                        "1" = "American values",
                        "2" = "Border control",
                        "3" = "Nationalisme",
                        "4" = "Trade",
                        "5" = "US economy",
                        "6" = "Election topics",
                        "7" = "Media/Fake news",
                        "8" = "Trump in the white house",
                        "9" = "The Trump witch hunt")) %>% 
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 3, scales = "free") 


```
# Sentiment 

Sentiment analysis is created to find the 5 most positive and negative tweets by Donald. The afinn lexicon is used which results in a either negative or positive score, each word takes a value between 5 and -5 depending on how positive or negative the word is. The 10 tweets are listed in the table below. 

```{r message=FALSE, warning=FALSE}
k <- 5

trump_sentiment <- trump_tidy %>%  
inner_join(get_sentiments("afinn")) %>% 
group_by(ID) %>% 
summarise(sentiment = sum(value)) 

df_n<-trump_tweet %>% 
  filter(ID %in% (trump_sentiment %>% 
  arrange(sentiment)%>% 
    select(ID) %>% 
  head(k) %>% 
  use_series(ID)  )) %>% 
  left_join((trump_sentiment %>% 
  arrange(sentiment) %>% 
  head(k) ), by = "ID") %>% 
  rename("Tweet text" = text)%>% 
  select(-date) %>% 
  arrange(sentiment)

df_p<-trump_tweet %>% 
  filter(ID %in% (trump_sentiment %>% 
  arrange(desc(sentiment))%>% 
    select(ID) %>% 
  head(k) %>% 
  use_series(ID)  )) %>% 
  left_join((trump_sentiment %>% 
  arrange(desc(sentiment)) %>% 
  head(k)), by = "ID") %>% 
  rename("Tweet text" = text)%>% 
  select(-date) %>% 
  arrange(desc(sentiment))


kable(cbind(df_n,df_p)) %>% 
  kable_styling(c("condensed","bordered","striped","hover")) %>% 
 add_header_above(c("Negative" = 3, "Positive" =3))
```

Here the nrc lexicon is used instead, it is selected to summarize the most used words related to fear. Seems Donald is afraid of witches ;).

```{r message=FALSE, warning=FALSE}
trump_tidy %>% 
  inner_join(get_sentiments("nrc") %>% filter(sentiment == "fear")) %>% 
  count(word, sort= TRUE) %>% 
  head()
```

The below graph shows the aggregated sentiment of each week from January 2017 - December 2018. He seems to have had more positive weeks than negative (maybe this have changed in 2019?).

```{r message=FALSE, warning=FALSE}
trump_sentiment_p <- trump_tidy %>%  
  inner_join(get_sentiments("afinn")) %>% 
  group_by(date) %>% 
  summarise(sentiment = sum(value)) %>% 
   mutate(mood = ifelse(sentiment >= 0, "positive","negative"))

trump_sentiment_p %>% 
  group_by(week(date), year(date)) %>% 
  summarize(sentiment=sum(sentiment),
            mood = ifelse(sentiment >= 0, "positive","negative"),
            date = first(date)) %>% 
  ggplot(aes(date, sentiment, fill=mood, color=mood)) +
  geom_col(width=1) +
  geom_point() +
  theme_light() +
  scale_color_manual(values = c("positive" = 'blue', "negative" = 'red')) + 
  scale_fill_manual(values = c("positive" = 'blue', "negative" = 'red')) + 
  geom_smooth(aes(group=0), color = "black", se=F)
```
# GloVe

The GloVe model is a algorithm used in unsupervised machine learning. The model creates vector representation of each words and uses these to measure similairy. 
To perform the GloVe the a corpus object has to be created. Afterwards the data is tokenized, turned into a Document-Feature-Matrix and then turned into a feature-co-occurence matrix.

```{r}
trump_corp <- trump_tweet %>%  corpus(docid_field = "ID", text_field = "text")

trump_toks <- tokens(trump_corp, what = "word") %>%
  tokens_tolower() %>%
  tokens(remove_punct = TRUE, 
         remove_symbols = TRUE)

trump_dfm <- dfm(trump_toks, verbose = TRUE) %>%
  dfm_trim(min_termfreq = 5) %>%
  featnames()

trump_fcm <- fcm(trump_toks, 
                 context = "window", 
                 count = "weighted", 
                 weights = 1 / (1:5), 
                 tri = TRUE)

glove <- GlobalVectors$new(word_vectors_size = 50, vocabulary = featnames(trump_fcm), x_max = 10)

trump_word_vectors <- fit_transform(trump_fcm, glove, n_iter = 20)
```


```{r}
trump_word_vectors %<>% as.data.frame() %>%
  rownames_to_column(var = "word") %>% 
  as_tibble()
trump_word_vectors %>% head()
```

```{r}
trump_tidy2 <- trump_toks %>% 
  dfm() %>% 
  tidy()

trump_vectors <- trump_tidy2 %>%
  inner_join(trump_word_vectors, by = c("term" = "word"))

trump_vectors %>% head()
```

```{r message=FALSE, warning=FALSE}
trump_vectors %<>%
  select(-term, -count) %>%
  group_by(document) %>%
  summarise_all(mean)

trump_vectors_umap <- umap(trump_vectors %>% column_to_rownames("document"), 
                       n_neighbors = 15, 
                       metric = "cosine", 
                       min_dist = 0.01, 
                       scale = TRUE,
                       verbose = TRUE) %>% 
  as.data.frame()

trump_vectors_umap %>% 
  ggplot(aes(x = V1, y = V2)) + 
  geom_point(shape = 21, alpha = 0.25) 
```

# ML 

```{r}
SP500 <- read_csv("https://raw.githubusercontent.com/Graverz/M2-Project-Trump/master/SP500.csv", col_types = cols(SP500 = col_number()))
SP500$DATE <- as_date(SP500$DATE)
SP500 <- SP500 %>% rename(date="DATE")
SP500$Year <- year(SP500$date)

#Data imputation
Imputation <- mice(SP500, m=5, maxit = 50, method = 'pmm', seed = 500)
SP500 <- complete(Imputation,1)

SP500$Procent <- (SP500$SP500-lag(SP500$SP500))/lag(SP500$SP500)*100
SP500 <- SP500 %>% filter(Year==2017 | Year==2018)  
hist(SP500$Procent)

SP500$Class1 <- ifelse(-0.2<SP500$Procent & SP500$Procent<0.2,"Niether",ifelse(SP500$Procent>=0.2, "Positive", ifelse(SP500$Procent<=-0.2, "Negative", "")))
SP500$Class1 <- as.factor(SP500$Class1)
```

```{r}
model_data <- trump_tidy %>% 
              left_join(SP500, by="date")
model_data <- model_data %>% filter(SP500!=" ")

class_join <- model_data %>% 
  select(ID, Class1) %>% unique()

trump_tweet1 <- trump_tweet %>% 
  left_join(class_join, by= "ID") %>% 
  filter(Class1!="")
```

```{r}
model_split <- trump_tweet1 %>% select(ID) %>% initial_split()
train_data <- training(model_split)
test_data <- testing(model_split)
```

```{r}
sparse_word <- model_data %>% 
  count(ID, word) %>% 
  inner_join(train_data) %>% 
  cast_sparse(ID,word,n)

dim(sparse_word)
```

```{r}
word_rowname <- as.integer(rownames(sparse_word))
joined <- data.frame(ID=word_rowname) %>% 
  left_join(trump_tweet1) %>% 
  select(ID, Class1)
```
```{r}
model <- cv.glmnet(sparse_word, joined$Class1, family="multinomial", keep=T)
```

```{r}
coefs <- model$glmnet.fit %>%
  tidy() %>%
  filter(lambda == model$lambda.1se)

classifications <- model_data %>%
  inner_join(test_data) %>%
  inner_join(coefs, by = c("word" = "term")) %>%
  group_by(class, ID) %>%
  summarize(score = sum(estimate)) %>%
  mutate(probability = plogis(0 + score))

cla <- classifications %>% 
  group_by(ID) %>% 
  filter(probability == max(probability)) %>% 
  left_join(trump_tweet1, by="ID")

result <- table(cla$class, cla$Class1)
result
c0 <- result[1,1]/sum(result[1,])
c1 <- result[2,2]/sum(result[2,])
c2 <- result[3,3]/sum(result[3,])
cA <- sum(result[1,1]+result[2,2]+result[3,3])/sum(result)
result2 <- as.data.frame(c(c0,c1,c2,cA))
result2 <- result2 %>% rename(Result = "c(c0, c1, c2, cA)")
rownames(result2) <- c("Negative", "Niether", "Positive", "Overall")
result2
```

```{r}
set.seed(17102019)

prop <- trump_tweet1 %>% 
  group_by(Class1) %>% 
  summarize(n = n())%>%
  mutate(freq = n / sum(n))

ran <- table(cla$class, sample(c("Negative", "Positive", "Niether"), nrow(cla), replace = T, prob = prop$freq))
ran
```

```{r}
ran <- rbind(spec(ran), precision(ran), accuracy(ran), recall(ran), npv(ran))
res <- rbind(spec(result), precision(result), accuracy(result), recall(result), npv(result))

res$model <- "Elastic Net"
ran$model <- "Random Assignment"
df <- rbind(res, ran)

ggplot(df, aes(.metric, .estimate, fill = reorder(model, desc(.estimate)))) + 
  geom_col(position = "dodge", width = 0.6) + 
  scale_fill_brewer(palette="Paired") + 
  labs(title = "Model performance", 
       subtitle = "", 
       fill = "Predictive Model")
```

```{r}
model$glmnet.fit %>%
  tidy()  %>% 
  filter(lambda == model$lambda.1se) %>%
  group_by(estimate > 0, class) %>%
  top_n(10, abs(estimate)) %>% 
  ggplot(aes(reorder_within(term, estimate, class), estimate, fill = estimate > 0)) +
    geom_col(alpha = 0.8, show.legend = FALSE, width=0.8) +
    coord_flip() +
  geom_hline(aes(yintercept=0)) +
    scale_x_reordered() +
    scale_fill_brewer(palette = "Paired") +
    labs( title = "Coefficients for the heighest weighted words in the penalized multinomial logistic regression model",
          subtitle = "", 
          x=NULL, y=NULL) +
  scale_y_continuous(limits=c(-1.5,1.5), breaks=seq(-1,1,by=0.5)) +
    facet_wrap(~class, scales="free_y")

ff %>% group_by(class) %>% count()
